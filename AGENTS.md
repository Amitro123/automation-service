# ğŸ¤– AGENTS.md - GitHub Automation Agent Instructions

Instructions for AI coding agents (Windsurf, Cursor, GitHub Copilot, etc.) working on this repository.

## ğŸ¯ Project Mission

**Single responsibility**: React to GitHub push events â†’ run 3 parallel LLM-powered tasks:
1. **Code Review** â†’ post intelligent feedback (comments/issues)
2. **README Update** â†’ detect changes â†’ create PR with docs
3. **Spec Update** â†’ append structured progress log

**Success metric**: After every push â†’ repo has fresh review + updated docs + progress log.

## ğŸ“ Project Structure

src/automation_agent/ # Core package
â”œâ”€â”€ __init__.py
â”œâ”€â”€ main.py # Flask entry point (python -m automation_agent.main)
â”œâ”€â”€ main_api.py # FastAPI entry point (NEW)
â”œâ”€â”€ api_server.py # FastAPI server with Dashboard API (NEW)
â”œâ”€â”€ webhook_server.py # Flask webhook endpoint
â”œâ”€â”€ orchestrator.py # Coordinates 4 parallel tasks
â”œâ”€â”€ session_memory.py # Session Memory Store (NEW)
â”œâ”€â”€ code_reviewer.py # LLM-powered code analysis
â”œâ”€â”€ code_review_updater.py # Persistent review logging
â”œâ”€â”€ readme_updater.py # Smart README updates from diffs
â”œâ”€â”€ spec_updater.py # Progress documentation
â”œâ”€â”€ github_client.py # GitHub API wrapper
â”œâ”€â”€ llm_client.py # OpenAI/Anthropic/Gemini abstraction
â”œâ”€â”€ config.py # .env loading + validation
â””â”€â”€ utils.py # Shared utilities

tests/ # pytest tests (mock external services)
dashboard/ # React + Vite dashboard
â”œâ”€â”€ App.tsx # Main dashboard component
â”œâ”€â”€ services/apiService.ts # FastAPI client
â””â”€â”€ components/ # UI components
run_api.py # FastAPI server launcher (NEW)
.env.example # Configuration template
requirements.txt # Dependencies
README.md # User documentation
spec.md # Product spec + progress log


## ğŸš€ Setup & Run

Install
git clone https://github.com/Amitro123/GithubAgent.git
cd GithubAgent
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

Configure
cp .env.example .env

Edit .env: GITHUB_TOKEN, OPENAI_API_KEY, etc.
Run locally
export PYTHONPATH=$PYTHONPATH:$(pwd)/src # Linux/Mac
python -m automation_agent.main # http://localhost:8080/

**Health check**: `curl http://localhost:8080/` â†’ `{"status": "ok"}`

## ğŸ› ï¸ Core Workflow (NEVER BREAK THIS)

GitHub Push Event â†’ webhook_server.py

Verify HMAC signature â†’ extract diff/commit SHA

orchestrator.py â†’ run 3 tasks IN PARALLEL:
â†³ code_reviewer.py â†’ post comment/issue
â†³ readme_updater.py â†’ create PR (if changes)
â†³ spec_updater.py â†’ append to spec.md

Log results + GitHub artifacts created
**All tasks log execution to `session_memory.py`**

## ğŸ“‹ Agent Task Priorities

When working on this repo, focus on these **in order**:

### 1. **Core Flow Reliability** (Highest priority)
âœ… Make webhook â†’ orchestrator â†’ GitHub side-effects robust
âœ… Add retry logic for transient API failures
âœ… Make tasks idempotent (handle webhook retries)
âœ… Improve error handling + logging


### 2. **LLM Output Quality**
âœ… Better prompts for code review (actionable, structured)
âœ… Smarter README change detection (functions/classes/APIs)
âœ… spec.md entries: summary + decisions + next steps
âœ… Chunk large diffs appropriately


### 3. **Configuration & Extensibility**
âœ… Add multi-LLM support (Gemini, local models)
âœ… Per-branch policies (stricter on main)
âœ… Agent platform integration (Windsurf/AntiGravity hooks)


## ğŸ§ª Testing Rules

**ALWAYS test changes with:**
1. Unit tests (mock everything external)
pytest tests/ -v

2. Manual end-to-end
echo "test" >> test.txt
git add test.txt && git commit -m "test automation" && git push

text

**Mock these in tests:**
- `github_client.py` (GitHub API)
- `llm_client.py` (OpenAI/Anthropic)  
- `requests` (webhook simulation)
- `session_memory.py` (Persistence layer)

## ğŸ’» Coding Standards

âœ… DO: Type hints + Google docstrings
def analyze_diff(diff: str, context: Dict[str, str]) -> str:
"""Analyzes git diff and returns structured review.

Args:
    diff: Raw git diff content
    context: File contents around changes
    
Returns:
    Markdown formatted review string
"""
pass
âŒ NEVER: print(), hardcoded values, missing types
text

**Dependencies**: Add to `requirements.txt`, never `pip freeze`.

## ğŸ”’ Security Rules

âŒ NEVER log:

GITHUB_TOKEN, API keys

Full git diffs (may contain secrets)

Raw webhook payloads

âœ… ALWAYS:

Verify webhook HMAC signature

Use minimal GitHub token scopes

Validate/sanitize LLM outputs before posting

Run `bandit -r src/` to check for security issues before pushing code


## ğŸ¯ Current spec.md Tasks

Read `spec.md` first, then prioritize:
1. âœ… Core functionality working
2. âœ… Comprehensive testing (Phase 3) - **99/99 tests passing, 100% coverage**
3. âœ… FastAPI + Dashboard Integration
4. âœ… Session Memory & Architecture Diagram
5. ğŸš€ E2E Testing with ngrok
6. ğŸš€ Deployment readiness (Phase 4) - Docker + CI/CD

## ğŸš« DON'T TOUCH (Unless Requested)

âŒ Don't change webhook payload format
âŒ Don't remove parallel task execution
âŒ Don't hardcode config values
âŒ Don't use print() for logging
âŒ Don't make real API calls in tests


## ğŸ“ˆ Success Metrics for Agents

Your changes are successful if:
âœ… pytest passes 100%
âœ… Local webhook server starts cleanly
âœ… Test push â†’ 3 tasks complete â†’ GitHub artifacts created
âœ… Logs are structured + no secrets exposed
âœ… README/spec.md stay accurate after changes

### 4. CodeReviewUpdater
- **Purpose**: Maintains a persistent log of all code reviews in `code_review.md`.
- **Logic**:
  - Runs after `CodeReviewer` completes successfully.
  - Summarizes the full review into a concise entry (Score, Key Issues, Action Items).
  - Appends to `code_review.md`.
- **Rules**:
  - Never overwrite the log, always append.
  - If `code_review.md` is missing, create it.
  - Use `LLMClient.summarize_review` for consistency.
